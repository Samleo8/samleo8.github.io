<!DOCTYPE HTML>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<link rel="shortcut icon" href="../Images/favicon.ico">
<title>Research</title>
<link href="../main.css" rel="stylesheet" type="text/css">
<noscript>
  <style>
  	#noScriptShow{
		display:inline-block;
		height:100%;
		width:100%;
		position:fixed;
		top:0;
		bottom:0;
		background:#066;
		text-align:center;
		z-index:2147483640; /* no mobile takes precedence */
	}
	#noScriptCircle{
		width:300px;
		height:300px;
		padding:10px;
		position: absolute;
		top:0;
		bottom: 0;
		left: 0;
		right: 0;
		margin:auto;
		border-radius:1000px;
		background:#EEE;
		color:#066;
		text-align:center;
	}
	#noScriptCircle div{
		width:200px;
		height:200px;	
		
		position: absolute;
		top:0;
		bottom: 0;
		left: 0;
		right: 0;
		margin:auto;
	}
	#noScriptCircle h1{
		font-size:40px;
		margin-top:20px;	
		margin-bottom:5px;
	}
  </style>
</noscript>
<script src="../main.lib.js"></script>
<script src="../main.js"></script>
</head>
<body>
<!--Navigation-->
<div id="navList" class="disappear">
    <div class="navCircle circle circle-height disableSelect alt-colour" id="navCircle1">
        <span class="center">Navigation<br>Menu</span>
    </div>
    <a><div class="navCircle circle circle-height alt-colour" id="navCircle2">
        <span class="menu-icon"></span>
        <span class="center"></span>
    </div></a>
    <a><div class="navCircle circle circle-height" id="navCircle3">
        <span class="menu-icon"></span>
        <span class="center"></span>
    </div></a>
    <a><div class="navCircle circle circle-height" id="navCircle4">
        <span class="menu-icon"></span>
        <span class="center"></span>
    </div></a>
    <a><div class="navCircle circle circle-height alt-colour" id="navCircle5">
        <span class="menu-icon"></span>
        <span class="center"></span>
    </div></a>
</div>

<!--START MAIN-->
<div id="main-content">
    <h1 class="title">Research</h1>
    <h3 class="subtitle">
        Over the years, under my school's Science and Math Talent Programme (SMTP) as well as the Young Defence Scientist Programme (YDSP), I have undertaken several interesting projects outlined in this page. 
    </h3>
    
    <div class="table-of-contents"></div>
    
    <div class="content-section">
        <h1>
            Optically illuminated directional sensing for guidance and alignment systems
            <div class="hr-divider pad-bottom"></div>        
        </h1>
        
        <div class="sub-table-of-contents"></div>
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">Abstract</span>
        </h2>
        
        <p>Optical guidance and alignment systems offer commercial applications in smart-home, elderly-aid and prosthetic systems. Conventional solutions that employ digital image processing for guidance and alignment are resource inefficient and thus costly to implement.</p>
        <p>This project explored the feasibility of using a common laser pointer for low-power, cost effective optical guidance. In this vein, a compact electronic sensing system, capable of navigating a small land robot using a beam of laser light as a proof of concept, was developed. Primary emphasis was placed on the characterization, design and implementation of the electrical circuitry to facilitate accurate and reliable detection and processing of an optical signal from a laser point on the wall. The system comprised a laser pointer modulated at 511Hz using a discrete 555-timer chip; four low-cost photodiode sensors to detect the modulated light signature; and a comprehensive quadrature lock-in amplifier circuit to filter out ambient noise and undesired light interference. The demodulated analog signal was then digitized and sent to a microcontroller, where a self-developed algorithm was used to actuate the robot in the direction of the reflected light. </p>
        <p>The hardware prototype, containing the sensors, circuits, motors and a battery pack, was integrated onto a compact 500g, 15x15x20cm omnidirectional robotic platform for maneuverability. The final system implementation responded successfully to the direction of reflected light in our tests, with acceptable sensitivity and robust noise rejection. Our innovative system could potentially be adapted, with some enhancements, for guiding and directing smart-home robots, prosthetic devices, motorized wheelchairs or vehicle parking.</p>
        
        <p>This project is done in collaboration with DSO National Laboratories, under the kind mentorship of Dr Wee Keng Hoong and Mr Lee Jin Yu. </p>
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">Awards</span>
        </h2>
        
        <p>This project won the Gold award at the Singapore Science and Engineering Fair.</p>
        <p>It was also selected to represent Singapore at the 2016 Intel International Science and Engineering Fair (ISEF), held in Arizona from 8 May 16 to 13 May 16. Photos of my ISEF experience there can be found <a href="#0-isef-experience">below</a>.</p>
        
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">Video</span>
        </h2>
        <div class="text-center">
            <div class="spacing small"></div>
            <video width="960" height="540" controls="controls">
                <source src="ISEF_2016.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        
        <p>The video above shows the main features of the electrical circuit as demonstrated on a simple land robot. The robot is able to follow the pulsing (red) laser pointer with accuracy and speed. Unlike digital-based systems, it is also able to distinguish between the user's laser and other laser interference.</p>
        
        <p><i>In case you were wondering why it's called Yoda's Lightchaser, it's because, first of all, it's (mostly) green like Yoda and his lightsaber. Secondly, one of the possible applications of the circuit is for elderly-friendly robots. Lastly, controlling the robot seems like one is using "The Force", especially if the circuit is adapted for use with an infrared laser.</i></p>
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">ISEF Experience</span>
        </h2>
        
        <p>Below are: the poster used for the ISEF 2016 competition; the link to my reflections on the experience; and a photo gallery!</p>
        
        <div class="spacing tiny"></div>
        <div  class="content-section-split">
            <div>
                <img src="../Images/isef-poster.png" width="706px" height="1000px">
            </div>
            <div class="text-center">
                <a href="https://sites.google.com/site/samuelleongportfolio/6-my-lea/isef-2016-experience" class="link-button gen-btn pad" target="_blank">
                    Link to Reflections
                </a>
                
                <a href="#gallery-isef" class="link-button gen-btn pad">
                    Photo Gallery below
                </a>
            </div>
        </div>
        <div class="spacing tiny"></div>
        
        <div id="gallery-isef" class="photoGallery flex-container gallery-isef">
            
        </div>
        
    </div>
    
    <div class="content-section">
        <h1>
            Analysis of multimodal interactions for simultaneous spatial and cognitive tasks
            <div class="hr-divider pad-bottom"></div>        
        </h1>
        
        <div class="sub-table-of-contents"></div>
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">Abstract</span>
        </h2>
        <p>With the advent and increasing popularity of new methods of Human Computer Interaction (HCI) such as eye-tracking, speech and gesture interaction modes, comes the question of the benefits these input modes bring. This project aims to study how multimodal interaction methods can reduce the overall workload and improve performance of the user whilst interacting with complex systems.</p>
        
        <p>The objective of the project was to compare several combinations of input methods and their effect on the overall workload and performance of the user whilst simultaneously performing 2 spatial tasks and 1 cognitive task. </p>
        
        <p>To achieve this, a simulation was developed as the experimental platform. The simulation recreates a combat situation that would require users to defend themselves and attack hostile targets, both of which required spatial localization. Changing of ammunition types, a cognitive task, was also added to increase the difficulty of multi-tasking.</p>
        
        <p>An experiment involving 12 participants across different age groups was conducted with 3 distinct interaction modes, assigned in random order. The first interaction mode is operated via mouse and keyboard; the second employs touch-screen, keyboard, and speech input; and the final interaction mode involves eye-tracking, keyboard, and speech input.</p>
        
        <p>This project is a component of the Research@YDSP module and is a collaborative effort between a team of Hwa Chong Institution students (including myself) and DSO National Laboratories.</p>
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">Awards</span>
        </h2>
        
        <p>We were selected to present our project to the then Minister of State for Defence, Mr Maliki Osman, who thoroughly enjoyed the live demonstration!</p>
        
        <p>The <a href="#1-preliminary-project">preliminary project</a> also obtained High Distinction at the Hwa Chong Projects' Competition</p>
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">Experiment Video</span>
        </h2>
        <div class="text-center">
            <div class="spacing small"></div>
            <video width="960" height="540" controls="controls">
                <source src="IFocus1ExptVideo.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
        <p>As seen in the video, an experiment was conducted involving 12 participants across different age groups.</p>
        <span>There were 3 distinct interaction modes, assigned in random order:</span>
        <ol>
            <li>Mouse and Keyboard</li>
            <li>Touch-screen, Keyboard, and Speech input</li>
            <li>Eye-tracking, Keyboard, and Speech input</li>
        </ol>
        
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">Experiment Game</span>
        </h2>
        <p>As part of the experiment, the participants were required to play a Flash game as shown below. </p>
        <p>The game involved heavy multitasking, with several tasks the user needed to do at once: </p>
        <ol>
            <li>Aiming the turret</li>
            <li>Shooting</li>
            <li>Changing ammo</li>
            <li>Defending with the shield</li>
        </ol>
        
        <div class="gameScreenshotHolder flex-container games-ifocus1">
        
        </div>
        
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">Preliminary Project</span>
        </h2>
        
        <p>This project was actually an extension of a preliminary project, done also in collaboration with DSO National Laboratories. This preliminary project, "I-Focus", was awarded a High Distinction at the Hwa Chong Projects Competition 2014.</p>
        
        <p>We conducted this project to explore the effectiveness, efficiency and intuitiveness of eye-tracking as a replacement for the mouse. To do this, we created a <a href="https://samleo8.github.io/eye-tracking-website" target="_blank">website</a> specifically suited for eye-tracking (of which the template that I coded has been adapted for my personal website). I also made a downloadable Chrome Extension which turns any website into an eye-tracking-friendly one! </p>
            
        <p>We also created a Flash game (programmed by myself) to compare the efficiency of eye-tracking with that of other interaction methods like the mouse. It can be played below on a browser which supports flash:</p>
        <div class="gameScreenshotHolder flex-container games-ifocus2">
        
        </div>
        
        <h2 class="less-margin-bottom">
            <span class="soft-border-underline">The Experience</span>
        </h2>
        
        <p>Below is a poster highlighting our key observations, which we used to present to the then Minister of State for Defence, Mr Maliki Osman, at the 2015 YDSP Congress.</p>
        <p>There is also a photo gallery below highlighting some key moments at the YDSP Congress event.</p>
        <div class="spacing tiny"></div>
        <div class="content-section-split">
            <img src="../Images/ifocus-poster.jpg" width="706px" height="1000px">
            <div class="text-center">
                <a href="https://sites.google.com/site/samuelleongportfolio/6-my-lea/dso-project-reflections" class="link-button gen-btn pad" target="_blank">
                    Link to Reflections
                </a>
                
                <a href="#gallery-ifocus" class="link-button gen-btn pad">
                    Photo Gallery below
                </a>
                
                <a href="https://samleo8.github.io/eye-tracking-website" class="link-button gen-btn pad" target="_blank">
                    Link to Website
                </a>
            </div>
        </div>
        <div class="spacing small"></div>
        
        <div id="gallery-ifocus" class="photoGallery flex-container gallery-ifocus"></div>
    </div>
    
    <div class="spacing large"></div>
</div>

<div></div>
<!--END MAIN-->
    
<!--Options-->
<div id="optionsCircle" class="circle circle-height disableSelect" onmouseover="optionsHover(this)" onmouseout="optionsUnhover(this)" onclick="optionsClick(this)">
    <span class="" id="optionsCircleContent">
        <h1>Options</h1>
	</span>
</div>

<!--Options Box-->
<div id="optionsBox" class="invertedColor disableSelect">
    <div id="optionsBoxHeader">
        <div class="optionsTitle"><h1>Options</h1></div>
        <div class="optionsIconsHolder">
            <div class="optionsIcons"><span class="icon-home"></span></div>
            <div class="optionsIcons"><span class="icon-reset"></span></div>
        </div>
    </div>
    <hr>
    <div id="optionsBoxContent">
        <div id="optionsMenuHide" onClick="toggleOptions('menuHide',this)" class="options-btn unchecked">Show Menu at Start</div>
        <div id="optionsMenuType" onClick="toggleOptions('menuTypeToggle',this)" class="options-btn unchecked disabled">Tab-Style Menu</div>

        <div class="options-btn" id="closeOptionsBtn" onClick="closeOptions()">Close and Save</div>
        <div id="optionsBoxHide" onClick="toggleOptions('boxHide',this)" class="options-btn unchecked">Hide options on launch</div>
    </div>
</div>
    
<!--Error Circles--->
<div id="noScriptShow">
    <div id="noScriptCircle" class="circle">
    	<div>
            <h1>Oops.</h1>
            <p>This website will not work without JavaScript enabled.</p>
            <p>Please enable JavaScript in your browser.</p>
        </div>
    </div>
</div>
    
</body>
</html>